<<<<<<< HEAD
# ğŸš€ Web Scraper AvanÃ§ado

Sistema avanÃ§ado de scraping web com interface grÃ¡fica para extraÃ§Ã£o e documentaÃ§Ã£o de sites.

## âœ¨ Funcionalidades

### ğŸ¯ Principais Recursos

- **Scrapy + Selenium**: CombinaÃ§Ã£o poderosa para sites estÃ¡ticos e dinÃ¢micos
- **Interface GrÃ¡fica**: FÃ¡cil de usar com tkinter moderno
- **Multi-mÃ©todo de ExtraÃ§Ã£o**:
  - Selenium para conteÃºdo JavaScript/dinÃ¢mico
  - Scrapy para crawling eficiente
  - Docling para conversÃ£o em Markdown
  - BeautifulSoup como fallback
- **Sistema Inteligente de Cache**: Evita reprocessar URLs jÃ¡ visitadas
- **Tratamento Robusto de Erros**: Retry automÃ¡tico e fallbacks mÃºltiplos
- **Controle de Profundidade**: Limite configurÃ¡vel de pÃ¡ginas
- **Filtros Inteligentes**: Ignora automaticamente arquivos binÃ¡rios e pÃ¡ginas irrelevantes

### ğŸ“Š Recursos AvanÃ§ados

- âœ… **RequisiÃ§Ãµes AssÃ­ncronas**: AtÃ© 8 requisiÃ§Ãµes simultÃ¢neas
- âœ… **DeduplicaÃ§Ã£o**: URLs processadas apenas uma vez
- âœ… **Metadados Persistentes**: Salva progresso entre execuÃ§Ãµes
- âœ… **Ãndice AutomÃ¡tico**: Gera INDEX.md com todos os arquivos
- âœ… **Logs Detalhados**: Acompanhamento em tempo real
- âœ… **Respeito ao robots.txt**: Crawling Ã©tico
- âœ… **User Agent Moderno**: Evita bloqueios

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Instalar DependÃªncias

```bash
pip install -r requirements.txt
```

### 2. Instalar ChromeDriver

O Selenium requer o ChromeDriver. Baixe em: https://chromedriver.chromium.org/

Ou use webdriver-manager (jÃ¡ incluÃ­do em requirements.txt):
```python
from webdriver_manager.chrome import ChromeDriverManager
```

## ğŸ® Uso

### Interface GrÃ¡fica

```bash
python interface.py
```

**Campos da Interface:**

1. **URL do Site**: URL completa do site a ser raspado
2. **Nome da Pasta**: Nome da pasta dentro de `DOCUMENTAÃ‡ÃƒO/`
3. **Limite de PÃ¡ginas**: MÃ¡ximo de pÃ¡ginas a processar (1-1000)
4. **Usar Selenium**: â˜‘ï¸ para sites com JavaScript

### Modo ProgramÃ¡tico

```python
from scrapper import SimpleWebScraper

# Criar scraper
scraper = SimpleWebScraper(
    url="https://exemplo.com",
    use_selenium=True,  # Usar Selenium para conteÃºdo dinÃ¢mico
    max_pages=100       # Limitar a 100 pÃ¡ginas
)

# Executar
scraper.run()
```

## ğŸ“ Estrutura de SaÃ­da

```
DOCUMENTAÃ‡ÃƒO/
â””â”€â”€ nome_da_pasta/
    â”œâ”€â”€ INDEX.md              # Ãndice geral
    â”œâ”€â”€ .metadata.json        # Metadados (cache)
    â”œâ”€â”€ pagina_1.md
    â”œâ”€â”€ pagina_2.md
    â””â”€â”€ ...
```

### Formato dos Arquivos

Cada arquivo `.md` contÃ©m:

```markdown
# TÃ­tulo da PÃ¡gina

**Fonte:** https://exemplo.com/pagina
**Data:** 2025-10-05 14:30:00

================================================================================

[ConteÃºdo extraÃ­do em Markdown]
```

## âš™ï¸ ConfiguraÃ§Ãµes

### scrapper.py (linhas 28-42)

```python
custom_settings = {
    'ROBOTSTXT_OBEY': True,           # Respeitar robots.txt
    'CONCURRENT_REQUESTS': 8,         # RequisiÃ§Ãµes simultÃ¢neas
    'DOWNLOAD_DELAY': 1,              # Delay entre requisiÃ§Ãµes (seg)
    'RETRY_TIMES': 5,                 # Tentativas em caso de erro
    'DEPTH_LIMIT': 5,                 # Profundidade mÃ¡xima
    'DOWNLOAD_TIMEOUT': 30,           # Timeout de download (seg)
}
```

### Filtros de URL (linhas 77-87)

URLs ignoradas automaticamente:
- `/tag/`, `/category/`, `/search`
- `/login`, `/register`, `/cart`
- PaginaÃ§Ã£o (`?page=`)
- Ã‚ncoras (`#`)

### ExtensÃµes Ignoradas (linhas 70-76)

- Imagens: png, jpg, jpeg, gif, svg, ico
- Documentos: pdf, zip, rar, tar, gz
- ExecutÃ¡veis: exe, dmg, pkg, deb, rpm
- MÃ­dia: mp4, avi, mov, mp3, wav
- Assets: css, js, woff, ttf

## ğŸ”§ PersonalizaÃ§Ã£o

### Adicionar Filtros Customizados

```python
# Em scrapper.py, linha 77
deny=[
    r'/tag/',
    r'/category/',
    r'/seu-filtro-aqui/',  # Adicione aqui
]
```

### Modificar Selenium Options

```python
# Em scrapper.py, linha 105-121
chrome_options.add_argument('--seu-argumento')
```

### Ajustar ExtraÃ§Ã£o de ConteÃºdo

```python
# Em scrapper.py, linha 229-230
# Modificar quais elementos remover
for element in soup(['script', 'style', 'nav', 'footer']):
    element.decompose()
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Selenium nÃ£o funciona

1. Verifique se o ChromeDriver estÃ¡ instalado
2. Verifique a versÃ£o do Chrome vs ChromeDriver
3. O scraper continuarÃ¡ funcionando sem Selenium (apenas Scrapy)

### Timeout/Erro de ConexÃ£o

- Aumente `DOWNLOAD_TIMEOUT` em `custom_settings`
- Reduza `CONCURRENT_REQUESTS`
- Aumente `DOWNLOAD_DELAY`

### PÃ¡ginas vazias

- Ative o Selenium para sites com JavaScript
- Verifique se o site nÃ£o bloqueia bots
- Ajuste o User Agent

### Muitas pÃ¡ginas ignoradas

- Revise os filtros em `deny=[]`
- Verifique `DEPTH_LIMIT`
- Aumente `max_pages`

## ğŸ“Š EstatÃ­sticas de Performance

- **Velocidade**: ~8 pÃ¡ginas/segundo (sem Selenium)
- **Velocidade c/ Selenium**: ~1-2 pÃ¡ginas/segundo
- **Uso de MemÃ³ria**: ~100-200 MB
- **CPU**: Moderado (multi-thread)

## ğŸ“ Logs e Debugging

### Ativar Logs Detalhados

```python
# Em scrapper.py, linha 35
'LOG_LEVEL': 'INFO',  # ou 'DEBUG'
```

### Arquivo de Metadados

O arquivo `.metadata.json` contÃ©m:
- URLs processadas (cache)
- Data da Ãºltima execuÃ§Ã£o
- URLs com erro
- Total de pÃ¡ginas

## ğŸ¤ Contribuindo

SugestÃµes e melhorias sÃ£o bem-vindas!

## âš ï¸ Avisos Legais

- Respeite os termos de serviÃ§o dos sites
- Respeite robots.txt (ativado por padrÃ£o)
- Use delays apropriados entre requisiÃ§Ãµes
- NÃ£o sobrecarregue servidores

## ğŸ“„ LicenÃ§a

MIT License - use livremente!

---

**Desenvolvido com â¤ï¸ usando Scrapy, Selenium, Docling e Tkinter**
=======
# simple-scrapper
>>>>>>> ad2508e8033bae63c0ad12a6ed97d154f25231d9
