# üöÄ Web Scraper Avan√ßado

Sistema avan√ßado de scraping web com interface gr√°fica para extra√ß√£o e documenta√ß√£o de sites.

## ‚ú® Funcionalidades

### üéØ Principais Recursos

- **Scrapy + Selenium**: Combina√ß√£o poderosa para sites est√°ticos e din√¢micos
- **Interface Gr√°fica**: F√°cil de usar com tkinter moderno
- **Multi-m√©todo de Extra√ß√£o**:
  - Selenium para conte√∫do JavaScript/din√¢mico
  - Scrapy para crawling eficiente
  - Docling para convers√£o em Markdown
  - BeautifulSoup como fallback
- **Sistema Inteligente de Cache**: Evita reprocessar URLs j√° visitadas
- **Tratamento Robusto de Erros**: Retry autom√°tico e fallbacks m√∫ltiplos
- **Controle de Profundidade**: Limite configur√°vel de p√°ginas
- **Filtros Inteligentes**: Ignora automaticamente arquivos bin√°rios e p√°ginas irrelevantes

### üìä Recursos Avan√ßados

- ‚úÖ **Requisi√ß√µes Ass√≠ncronas**: At√© 8 requisi√ß√µes simult√¢neas
- ‚úÖ **Deduplica√ß√£o**: URLs processadas apenas uma vez
- ‚úÖ **Metadados Persistentes**: Salva progresso entre execu√ß√µes
- ‚úÖ **√çndice Autom√°tico**: Gera INDEX.md com todos os arquivos
- ‚úÖ **Logs Detalhados**: Acompanhamento em tempo real
- ‚úÖ **Respeito ao robots.txt**: Crawling √©tico
- ‚úÖ **User Agent Moderno**: Evita bloqueios

## üì¶ Instala√ß√£o

### 1. Instalar Depend√™ncias

```bash
pip install -r requirements.txt
```

### 2. Instalar ChromeDriver

O Selenium requer o ChromeDriver. Baixe em: https://chromedriver.chromium.org/

Ou use webdriver-manager (j√° inclu√≠do em requirements.txt):
```python
from webdriver_manager.chrome import ChromeDriverManager
```

## üéÆ Uso

### Interface Gr√°fica

```bash
python interface.py
```

**Campos da Interface:**

1. **URL do Site**: URL completa do site a ser raspado
2. **Nome da Pasta**: Nome da pasta dentro de `DOCUMENTA√á√ÉO/`
3. **Limite de P√°ginas**: M√°ximo de p√°ginas a processar (1-1000)
4. **Usar Selenium**: ‚òëÔ∏è para sites com JavaScript

### Modo Program√°tico

```python
from scrapper import SimpleWebScraper

# Criar scraper
scraper = SimpleWebScraper(
    url="https://exemplo.com",
    use_selenium=True,  # Usar Selenium para conte√∫do din√¢mico
    max_pages=100       # Limitar a 100 p√°ginas
)

# Executar
scraper.run()
```

## üìÅ Estrutura de Sa√≠da

```
DOCUMENTA√á√ÉO/
‚îî‚îÄ‚îÄ nome_da_pasta/
    ‚îú‚îÄ‚îÄ INDEX.md              # √çndice geral
    ‚îú‚îÄ‚îÄ .metadata.json        # Metadados (cache)
    ‚îú‚îÄ‚îÄ pagina_1.md
    ‚îú‚îÄ‚îÄ pagina_2.md
    ‚îî‚îÄ‚îÄ ...
```

### Formato dos Arquivos

Cada arquivo `.md` cont√©m:

```markdown
# T√≠tulo da P√°gina

**Fonte:** https://exemplo.com/pagina
**Data:** 2025-10-05 14:30:00

================================================================================

[Conte√∫do extra√≠do em Markdown]
```

## ‚öôÔ∏è Configura√ß√µes

### scrapper.py (linhas 28-42)

```python
custom_settings = {
    'ROBOTSTXT_OBEY': True,           # Respeitar robots.txt
    'CONCURRENT_REQUESTS': 8,         # Requisi√ß√µes simult√¢neas
    'DOWNLOAD_DELAY': 1,              # Delay entre requisi√ß√µes (seg)
    'RETRY_TIMES': 5,                 # Tentativas em caso de erro
    'DEPTH_LIMIT': 5,                 # Profundidade m√°xima
    'DOWNLOAD_TIMEOUT': 30,           # Timeout de download (seg)
}
```

### Filtros de URL (linhas 77-87)

URLs ignoradas automaticamente:
- `/tag/`, `/category/`, `/search`
- `/login`, `/register`, `/cart`
- Pagina√ß√£o (`?page=`)
- √Çncoras (`#`)

### Extens√µes Ignoradas (linhas 70-76)

- Imagens: png, jpg, jpeg, gif, svg, ico
- Documentos: pdf, zip, rar, tar, gz
- Execut√°veis: exe, dmg, pkg, deb, rpm
- M√≠dia: mp4, avi, mov, mp3, wav
- Assets: css, js, woff, ttf

## üîß Personaliza√ß√£o

### Adicionar Filtros Customizados

```python
# Em scrapper.py, linha 77
deny=[
    r'/tag/',
    r'/category/',
    r'/seu-filtro-aqui/',  # Adicione aqui
]
```

### Modificar Selenium Options

```python
# Em scrapper.py, linha 105-121
chrome_options.add_argument('--seu-argumento')
```

### Ajustar Extra√ß√£o de Conte√∫do

```python
# Em scrapper.py, linha 229-230
# Modificar quais elementos remover
for element in soup(['script', 'style', 'nav', 'footer']):
    element.decompose()
```

## üêõ Solu√ß√£o de Problemas

### Selenium n√£o funciona

1. Verifique se o ChromeDriver est√° instalado
2. Verifique a vers√£o do Chrome vs ChromeDriver
3. O scraper continuar√° funcionando sem Selenium (apenas Scrapy)

### Timeout/Erro de Conex√£o

- Aumente `DOWNLOAD_TIMEOUT` em `custom_settings`
- Reduza `CONCURRENT_REQUESTS`
- Aumente `DOWNLOAD_DELAY`

### P√°ginas vazias

- Ative o Selenium para sites com JavaScript
- Verifique se o site n√£o bloqueia bots
- Ajuste o User Agent

### Muitas p√°ginas ignoradas

- Revise os filtros em `deny=[]`
- Verifique `DEPTH_LIMIT`
- Aumente `max_pages`

## üìä Estat√≠sticas de Performance

- **Velocidade**: ~8 p√°ginas/segundo (sem Selenium)
- **Velocidade c/ Selenium**: ~1-2 p√°ginas/segundo
- **Uso de Mem√≥ria**: ~100-200 MB
- **CPU**: Moderado (multi-thread)

## üìù Logs e Debugging

### Ativar Logs Detalhados

```python
# Em scrapper.py, linha 35
'LOG_LEVEL': 'INFO',  # ou 'DEBUG'
```

### Arquivo de Metadados

O arquivo `.metadata.json` cont√©m:
- URLs processadas (cache)
- Data da √∫ltima execu√ß√£o
- URLs com erro
- Total de p√°ginas

## ü§ù Contribuindo

Sugest√µes e melhorias s√£o bem-vindas!

## ‚ö†Ô∏è Avisos Legais

- Respeite os termos de servi√ßo dos sites
- Respeite robots.txt (ativado por padr√£o)
- Use delays apropriados entre requisi√ß√µes
- N√£o sobrecarregue servidores

## üìÑ Licen√ßa

MIT License - use livremente!

---

**Desenvolvido com ‚ù§Ô∏è usando Scrapy, Selenium, Docling e Tkinter**
